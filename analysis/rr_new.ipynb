{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43579668-7ddf-4e84-9eb2-3eb5b7d595d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.linear_model import Ridge as ridge\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from multiprocessing import Pool, Semaphore as sem, cpu_count\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cb96d9-93ea-4aa6-891b-d1da2505aa23",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'temperature_2m_max'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/bkelley/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'temperature_2m_max'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_673312/4198748861.py\", line 61, in process_header\n    data['target'] = data.shift(-1)[current_value]\n  File \"/home/bkelley/.local/lib/python3.9/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/home/bkelley/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'temperature_2m_max'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m64\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    115\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Print the final result for each header\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temperature_2m_max'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.linear_model import Ridge as ridge\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from multiprocessing import Pool, Semaphore, cpu_count\n",
    "import time\n",
    "\n",
    "global LOC, DAILY_FILEPATH, HOURLY_FILEPATH, headers, new_cols\n",
    "new_cols = [\n",
    "    'target', 'weather_code', 'temperature_2m', 'relative_humidity_2m', 'precipitation', \n",
    "    'rain', 'surface_pressure', 'cloud_cover', 'wind_speed_10m', 'wind_speed_100m', \n",
    "    'wind_direction_10m', 'wind_direction_100m', 'temperature_2m_K', 'surface_pressure_Pa', \n",
    "    'density', 'speed_of_sound'\n",
    "]\n",
    "LOC = '/home/bkelley/capstone'\n",
    "DAILY_FILEPATH='/home/bkelley/capstone/data_collection/weather/data/daily_weather_with_temp_avg.csv'\n",
    "HOURLY_FILEPATH = '/home/bkelley/capstone/data_collection/weather/data/cleaned_hourly_weather_data.csv'\n",
    "# Semaphore to limit the number of parallel processes\n",
    "sem = Semaphore(64)  # Limit to the number of CPU cores\n",
    "\n",
    "\n",
    "# generates predictions for data except first 5 years\n",
    "def backtest(data, model, predictors, start=365*5, step=70):\n",
    "    all_pred = []\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[:i, :]  # all rows up to i\n",
    "        test = data.iloc[i:(i+step), :]  # takes next step days to make predictions on\n",
    "        model.fit(train[predictors], train['target'])\n",
    "        preds = model.predict(test[predictors])\n",
    "        preds = pd.Series(preds, index=test.index)\n",
    "        combined = pd.concat([test['target'], preds], axis=1)\n",
    "        combined.columns = ['actual', 'prediction']\n",
    "        combined['diff'] = (combined['prediction'] - combined['actual']).abs()\n",
    "        all_pred.append(combined)\n",
    "    return pd.concat(all_pred)\n",
    "\n",
    "def pct_diff(old, new):\n",
    "    return (new - old) / old\n",
    "\n",
    "def compute_rolling(data, horizon, col):\n",
    "    label = f\"rolling_{horizon}_{col}\"\n",
    "    data[label] = data[col].rolling(horizon).mean()\n",
    "    data[f\"{label}_pct\"] = pct_diff(data[label], data[col])\n",
    "    return data\n",
    "\n",
    "def expand_mean(data):\n",
    "    return data.expanding(1).mean()\n",
    "\n",
    "# Function to process each header with alpha tuning\n",
    "# Adjust the stopping criteria and step size\n",
    "def process_header(current_value):\n",
    "    with sem:\n",
    "        result = []\n",
    "        data = pd.read_csv(HOURLY_FILEPATH, index_col='date')\n",
    "        # fill in missing data with last known value\n",
    "        data = data.ffill()\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "\n",
    "        data['target'] = data.shift(-1)[current_value]\n",
    "        data = data.ffill()\n",
    "\n",
    "        # Set initial alpha and other parameters\n",
    "        alpha = 0.1\n",
    "        step_size = 0.5  # Increase the step size\n",
    "        min_delta = 0.01  # Allow larger changes in MAE\n",
    "        max_iterations = 10_000  # Cap the iterations\n",
    "        previous_mae = float('inf')\n",
    "        best_alpha = alpha\n",
    "        best_mae = previous_mae\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            rr = ridge(alpha=alpha)\n",
    "            predictors = data.columns[~data.columns.isin([new_cols])]\n",
    "            predictions = backtest(data, rr, predictors)\n",
    "            mean_abs_error = mae(predictions['actual'], predictions['prediction'])\n",
    "\n",
    "            mae_diff = abs(previous_mae - mean_abs_error)\n",
    "\n",
    "            if mean_abs_error < best_mae:\n",
    "                best_mae = mean_abs_error\n",
    "                best_alpha = alpha\n",
    "\n",
    "            # If MAE change is below threshold, continue but stop after a set number of iterations\n",
    "            if mae_diff < min_delta and iteration > 100:\n",
    "                break\n",
    "\n",
    "            previous_mae = mean_abs_error\n",
    "            alpha += step_size\n",
    "            iteration += 1\n",
    "\n",
    "        # Return only the best result for each header\n",
    "        return f\"Best MAE for {current_value}: {best_mae:.6f} with optimal alpha: {best_alpha}\"\n",
    "\n",
    "# Main code with multiprocessing pool\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(DAILY_FILEPATH, index_col='date')\n",
    "    null_pct = data.apply(pd.isnull).sum()/data.shape[0]\n",
    "    valid_columns = data.columns[null_pct < 0.05]\n",
    "    data = data[valid_columns].copy()\n",
    "    data.columns = data.columns.str.lower()\n",
    "\n",
    "    data = data.ffill()\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    \n",
    "    headers = [col for col in data.columns if col not in ['weather_code']]\n",
    "\n",
    "    results = []\n",
    "    with Pool(64) as pool:\n",
    "        for result in pool.map(process_header, headers):\n",
    "            results.append(result)\n",
    "\n",
    "    # Print the final result for each header\n",
    "    print(\"\\n\".join(results))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nFinished processing all headers in {end_time - start_time:.2f} seconds.\")\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cb129-fa20-41f3-8693-b5dedebbccff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
